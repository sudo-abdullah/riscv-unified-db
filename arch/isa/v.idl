%version: 1.0

# Enumeration for LMUL (Vector Register Group Multiplier)
enum LMUL {
    M1 1     # Single vector register group (Default)
    M2 2     # Double vector register group
    M4 4     # Quadruple vector register group
    M8 8     # Octuple vector register group
}

enum Elen {
    ELEN32 32  #Elective Element Width 32
    ELEN64 64  #Elective Element Width 64
}

enum Vlen {
    Vlen128 128  #vector Length 128
    Vlen256 256  #vector Length 256
}

# Enumeration for SEW (Standard Element Width - Data type size per vector element)
enum Sew {
    Sew8 8    # 8-bit elements (1 byte per element)
    Sew16 16  # 16-bit elements (2 bytes per element)
    Sew32 32  # 32-bit elements (4 bytes per element - Common default)
    Sew64 64  # 64-bit elements (8 bytes per element)
}



# Vector Configuration Structure
struct VectorConfig {
    U32 vl;                 # Current vector length
    U32 vlmax;
    U32 Vlen;
    U32 Sew;                # Selected element width
    Bits<8> lmul;                # Vector register group multiplier
    Bits<8> mask_policy;         # 0: undisturbed, 1: agnostic
}




# Main Vector Arithmetic Operation
function vectorArithmetic {
    returns U32
    arguments
        U32 vs1,
        U32 vs2,
        U32 vs3,
        Bits<8> opcode,
        Bits<1> mask_bit,
        Bits<1> mask_instr_bit25
    description {
        Executes vector arithmetic operations while handling LMUL-aware indexing,
        mask/tail policies, and element-wise computations.
    }
    body {
        VectorConfig cfg;
        U32 result = perform_operation(vs1, vs2, opcode);
        if (mask_instr_bit25 == 1) {  # Masking enabled
            if (mask_bit==1) {  # Active element
                return result;
            } else {  # Masked element: apply mask policy
                return handle_mask_policy(vs3, cfg);
            }
        } else {  # Masking disabled
            return result;
        }
    }
}

# Performs the specified arithmetic operation between two vector elements
function perform_operation {
    returns U32
    arguments
        U32 vs1,
        U32 vs2,
        Bits<8> opcode
    description {
        Executes arithmetic operations based on the given opcode.
    }
    body {
        if (opcode == 0) {  # vadd: Addition
            return vs1 + vs2;
        } else if (opcode == 2) {  # vsub: Subtraction
            return vs2 - vs1;
        } else {
            # Handle other opcodes or return error value
            return 0;
        }
    }
}

# Slideup operation for both vslideup.vx and vslideup.vi
function perform_slideup {
    returns U32
    arguments
        U32 offset,
        list<U32> vs2,
        U32 index
    description {
        Slides elements of vs2 up by 'offset' positions.
        Returns 0 for elements that shift in from below.
        Used by both vslideup.vx (offset from scalar register) and vslideup.vi (offset immediate).
    }
    body {
        if (index < offset) {
            return 0;  # Inserted zero for positions below offset
        } else {
            return vs2[index - offset];  # Shifted element
        }
    }
}



# Mask Policy Handling
function handle_mask_policy {
    returns U32
    arguments
        U32 original_value,
        VectorConfig cfg
    description {
        Determines the value for masked elements based on mask policy.
        Returns the original value if undisturbed, or all 1s if agnostic.
    }
    body {
        if (cfg.mask_policy == 0) {  # Mask undisturbed
            return original_value;
        } else {  # Mask agnostic
            return 0xFFFFFFFF;
        }
    }
}

# Tail Element Handling
function handle_tail_elements {
    returns U32
    arguments
        U32 original_value

    description {
        Determines the value for tail elements based on the selected tail policy.
        Returns the original value if undisturbed, or all 1s if agnostic.
    }
    body {
        Bits<1> TU = 0;
        if (TU == 0) {  # Tail undisturbed
            return original_value;
        } else {  # Tail agnostic
            return 0xFFFFFFFF;  # All 1s pattern
        }
    }
}

# perform operation on vector elements
function process_vector_elements {
    returns U32
    arguments
        U32 vs1_data,
        U32 vs2_data,
        U32 vs3_data,
        Bits<1> vs0_mask_bit,
        U32 encoding
    description {
        Executes element-wise vector arithmetic using input operands and configuration settings.
        Incorporates masking logic and handles conditional execution based on mask bit 25.
        Checks the tail policy and applies either tail-agnostic or tail-undisturbed behavior accordingly.
        Returns the final processed 32-bit result after applying all vector rules.

        Implements tail policy when storing values in vector registers.
        Retains body Elements if within vector length (VL), otherwise applies tail policy.
    }
    body {

        U32 vl=4;
        U32 vlmax=4;
        U32 result = vectorArithmetic(
                vs1_data,
                vs2_data,
                vs3_data,
                encoding[31:26],
                vs0_mask_bit,
                encoding[25]
            );
        if (vlmax > vl) {  # body elements
            return result;
        }else {  # Tail element
            return handle_tail_elements(result);
        }
    }
}

function sew_selection {
    returns U32
    arguments
        U32 vs1_data,
        U32 vs2_data,
        U32 vs3_data,
        Bits<1> vs0_mask,
        U32 encoding
    description {
        Determines the standard element width (SEW) and performs vector operations accordingly.
        Based on SEW (32, 16, or 8 bits), splits the computation into appropriate element segments.
        Calls the vector processing function for each segment and aggregates the results.
        Returns the final result as a 32-bit value or a combination of smaller-width results.
    }
    body {

        if ($bits(Sew::Sew32)==32){
            U32 resultsew =  process_vector_elements(vs1_data, vs2_data, vs3_data, vs0_mask,encoding);
            return resultsew;
        }
        else if ($bits(Sew::Sew32)==16){
            Bits<16> resultsewa =  process_vector_elements(vs1_data, vs2_data, vs3_data, vs0_mask,encoding);
            Bits<16> resultsewb =  process_vector_elements(vs1_data, vs2_data, vs3_data, vs0_mask,encoding);
            return {resultsewa, resultsewb};
        }
        else if ($bits(Sew::Sew32)==8){
            Bits<8> resultsewa =  process_vector_elements(vs1_data, vs2_data, vs3_data, vs0_mask,encoding);
            Bits<8> resultsewb =  process_vector_elements(vs1_data, vs2_data, vs3_data, vs0_mask,encoding);
            Bits<8> resultsewc =  process_vector_elements(vs1_data, vs2_data, vs3_data, vs0_mask,encoding);
            Bits<8> resultsewd =  process_vector_elements(vs1_data, vs2_data, vs3_data, vs0_mask,encoding);
            return {resultsewa, resultsewb, resultsewc, resultsewd};
        }
        else{
            return resultsew;
        }
    }
}

function vslideup_operation {
    returns U32
    arguments
        U32 vs2_element,
        U32 vd_old_element,
        U64 offset,
        U32 index,
        Bits<1> mask_bit,
        VectorConfig cfg
    description {
        Handles vector slide-up operations with proper mask/tail policies.
    }
    body {
        U32 first_active = max(cfg.vstart, offset);

        if (index >= cfg.vl) {
            return handle_tail_elements(vd_old_element);
        }
        else if (mask_bit == 0) {
            return handle_mask_policy(vd_old_element, cfg);
        }
        else if (index < first_active) {
            return vd_old_element;
        }
        else {
            return vs2_element;
        }
    }
}

function vslidedown_operation {
    returns U32
    arguments
        U32 vs2_element,
        U32 vd_old_element,
        U64 offset,
        U32 index,
        Bits<1> mask_bit,
        VectorConfig cfg
    description {
        Handles vector slide-down operations with proper mask/tail policies.
        Returns 0 for source elements when i+offset >= VLMAX
    }
    body {
        # Source element selection
        U32 src_element = ((index + offset) < cfg.vlmax) ? vs2_element : 0;

        # Destination policy handling
        if (index >= cfg.vl) {
            return handle_tail_elements(vd_old_element);
        }
        else if (index < cfg.vstart) {
            return vd_old_element;
        }
        else if (mask_bit == 0) {
            return handle_mask_policy(vd_old_element, cfg);
        }
        else {
            return src_element;
        }
    }
}
function vslide1_operation {
    returns U32
    arguments
        U32 vs2_element,
        U32 vd_old_element,
        U32 scalar_value,    # From x/f register
        U32 index,
        Bits<1> mask_bit,
        VectorConfig cfg,
        Bits<2> mode         # 0:slide1up, 1:slide1down, 2:fslide1up, 3:fslide1down
    description {
        Handles all slide1 variants with proper scalar insertion and policies.
        Mode: 0=vslide1up, 1=vslide1down, 2=vfslide1up, 3=vfslide1down
    }
    body {
        # Common policy checks
        if (index >= cfg.vl) {
            return handle_tail_elements(vd_old_element);
        }
        else if (index < cfg.vstart) {
            return vd_old_element;
        }
        else if (mask_bit == 0) {
            return handle_mask_policy(vd_old_element, cfg);
        }

        # Mode-specific behavior
        if (mode[0] == 0) {  # slide1up variants
            if (index == max(cfg.vstart, 0)) {
                return (cfg.Sew > 32) ? sign_extend(scalar_value) : scalar_value;
            }
            else if (index > 0) {
                return vs2_element;  # vs2[index-1]
            }
        }
        else {  # slide1down variants
            if (index == (cfg.vl - 1)) {
                return (cfg.Sew > 32) ? sign_extend(scalar_value) : scalar_value;
            }
            else {
                return vs2_element;  # vs2[index+1]
            }
        }
        return vd_old_element;  # Fallthrough
    }
}

function vrgather_operation {
    returns U32
    arguments
        U32 vs2_element,     # Source vector element
        U32 index_value,     # Index value (from vs1/xreg/imm)
        U32 vd_old_element,  # Original destination value
        U32 index,           # Current element index
        Bits<1> mask_bit,
        VectorConfig cfg,
        Bits<2> mode         # 0:vv, 1:vx, 2:vi, 3:ei16
    description {
        Handles all vrgather variants with proper index bounds checking.
        Mode: 0=vv, 1=vx, 2=vi, 3=ei16
    }
    body {
        # Common policy checks
        if (index >= cfg.vl) {
            return handle_tail_elements(vd_old_element);
        }
        else if (index < cfg.vstart) {
            return vd_old_element;
        }
        else if (mask_bit == 0) {
            return handle_mask_policy(vd_old_element, cfg);
        }

        # Index bounds check
        if (index_value >= cfg.vlmax) {
            return 0;
        }

        # For vx/vi modes, verify we're accessing the requested index
        if (mode[1] == 1) {  # vx/vi modes
            if (index != 0 && index_value != index) {
                return vd_old_element;
            }
            return vs2_element;  # vs2[index_value] (pre-looked up)
        }
        else {  # vv/ei16 modes
            return vs2_element;  # vs2[index_value] (pre-looked up)
        }
    }
}

function vcompress_operation {
    returns U32
    arguments
        U32 vs2_element,     # Source vector element
        U32 vd_old_element,  # Original destination value
        U32 index,           # Current element index
        Bits<1> mask_bit,    # From vs1 mask register
        VectorConfig cfg,
        ref U32 write_ptr    # Tracks compressed position
    description {
        Handles vcompress operation with proper packing and policies.
        Returns tuple of (new_value, write_ptr_update).
    }
    body {
        # Illegal if vstart != 0 (per spec)
        if (cfg.vstart != 0) {
            $raise_illegal_instruction();
        }

        # Tail elements
        if (index >= cfg.vl) {
            return handle_tail_elements(vd_old_element);
        }

        # Active compression
        if (mask_bit == 1) {
            U32 result = vs2_element;
            write_ptr = write_ptr + 1;  # Increment write pointer
            return result;
        }
        else if (index < write_ptr) {
            # Keep already compressed elements
            return vd_old_element;
        }
        else {
            # Fill gaps with tail policy
            return handle_tail_elements(vd_old_element);
        }
    }
}

function vmv_s_x_operation {
    returns U32
    arguments
        U32 x_data,
        U32 vd_old_element,
        U32 index,
        VectorConfig cfg
    description {
        Moves scalar to vector element with sign-extension when SEW > XLEN.
    }
    body {
        if (index >= cfg.vl) {
            return handle_tail_elements(vd_old_element);
        }
        return (cfg.Sew > 32) ? sign_extend(x_data) : x_data;
    }
}

function vmv_x_s_operation {
    returns U32
    arguments
        U32 vs2_element,
        U32 x_old,
        VectorConfig cfg
    description {
        Moves vector element to scalar register with truncation when SEW > XLEN.
    }
    body {
        return (cfg.Sew > 32) ? (vs2_element & 0xFFFFFFFF) : vs2_element;
    }
}

# === UPDATED perform_arith ===
function perform_arith {
    returns U32
    arguments
        U32 vs1_data,
        U32 vs2_data,
        U32 vs3_data,
        Bits<1> vs0_mask_bit,
        U32 encoding
    description {
        Capable to handle Vector Permutation Istructions.
    }
    body {

        if (encoding[31:26] == 0x22 && encoding[25] == 1) {  # vcompress.vm
            static U32 write_ptr = 0;  # Persistent across elements

            U32 result = vcompress_operation(
                vs2_data,
                vs3_data,
                $element_index,
                vs1_data,  # Mask bit from vs1
                $get_config(),
                write_ptr
            );

            # Reset write pointer after last element
            if ($element_index == (VLMAX - 1)) {
                write_ptr = 0;
            }

            return result;
        }

        else if (encoding[31:26] == 0x20) {  # vrgather
            Bits<2> mode = {encoding[25], encoding[20]};  # Determine variant

            U32 index_val;
            if (mode == 0) {  # vv
                index_val = vs1_data;
            }
            else if (mode == 1) {  # vx
                index_val = X[encoding[19:15]];
            }
            else if (mode == 2) {  # vi
                index_val = encoding[19:15];
            }
            else {  # ei16
                index_val = vs1_data & 0xFFFF;  # 16-bit indices
            }

            return vrgather_operation(
                vs2_data,    # Source element (pre-indexed)
                index_val,   # Actual index value
                vs3_data,   # Original destination
                $element_index,
                vs0_mask_bit,
                $get_config(),
                mode
            );
        }

        else if (encoding[31:26] == 0x1C) {  # vslide1up/vfslide1up
            return vslide1_operation(
                vs2_data,
                vs3_data,
                X[encoding[19:15]],  # x/f register
                $element_index,
                vs0_mask_bit,
                $get_config(),
                {encoding[25], 0}    # mode[1:0]
            );
        }
        else if (encoding[31:26] == 0x1D) {  # vslide1down/vfslide1down
            return vslide1_operation(
                vs2_data,
                vs3_data,
                X[encoding[19:15]],  # x/f register
                $element_index,
                vs0_mask_bit,
                $get_config(),
                {encoding[25], 1}   # mode[1:0]
            );
        }
        else if (encoding[31:26] == 0x19) {  # vslidedown
            return vslidedown_operation(
                vs2_data,
                vs3_data,
                (encoding[25] ? X[encoding[19:15]] : encoding[19:15]),  # offset
                $element_index,
                vs0_mask_bit,
                $get_config()
            );
        }
        else if (encoding[31:26] == 0x18) {  # vslideup
            return vslideup_operation(
                vs2_data,
                vs3_data,
                (encoding[25] ? X[encoding[19:15]] : encoding[19:15]),  # offset
                $element_index,
                vs0_mask_bit,
                $get_config()
            );
        }
        else if (encoding[31:26] == 0x1A) {  # vmv.s.x
            return vmv_s_x_operation(
                X[encoding[19:15]],  # rs1
                vs3_data,
                $element_index,
                $get_config()
            );
        }
        else if (encoding[31:26] == 0x1B) {  # vmv.x.s
            return vmv_x_s_operation(
                vs2_data,
                vs3_data,
                $get_config()
            );
        }
        else {
            # Original arithmetic logic
            return sew_selection(vs1_data, vs2_data, vs3_data, vs0_mask_bit, encoding);
        }
    }
}
